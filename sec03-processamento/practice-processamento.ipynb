{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"practice-processamento.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# JAI 2022 - Curso de PLN\n","\n","---\n","\n","## Seção 1.3. Preparação de Dados Textuais"],"metadata":{"id":"ok3oWIsbZPWJ"}},{"cell_type":"markdown","source":["# Instalação de pacotes adicionais"],"metadata":{"id":"i93iN-U2Y69d"}},{"cell_type":"code","source":["%%capture\n","!pip install unidecode\n","!python -m spacy download pt_core_news_sm"],"metadata":{"id":"kLMmtoOC5y0j","executionInfo":{"status":"ok","timestamp":1659358121624,"user_tz":180,"elapsed":19539,"user":{"displayName":"Gustavo Coutinho","userId":"05179387076781997096"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Importações Necessárias"],"metadata":{"id":"VZu0HDCYY-xr"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"hqh_rtsql9yo","executionInfo":{"status":"ok","timestamp":1659358143304,"user_tz":180,"elapsed":5786,"user":{"displayName":"Gustavo Coutinho","userId":"05179387076781997096"}}},"outputs":[],"source":["import string\n","from unidecode import unidecode\n","\n","import numpy as np\n","\n","# Uso da biblioteca NLTK para estemização\n","from nltk.stem.snowball import PortugueseStemmer\n","\n","# Uso da biblioteca spaCy para lematização\n","import spacy"]},{"cell_type":"markdown","source":["# Padronização"],"metadata":{"id":"FT1ZxGGiZCBe"}},{"cell_type":"markdown","source":["A função ```standardize()``` irá\n","\n","*   Transformar todas as letras em letras minúsculas com a função ```lower()```\n","*   Remover todos os acentos e pontuações presentes em ```string.punctuation```\n","\n"],"metadata":{"id":"S1SKwWI_d2zg"}},{"cell_type":"code","source":["string.punctuation"],"metadata":{"id":"1rAY93nwfa22","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1659358183427,"user_tz":180,"elapsed":298,"user":{"displayName":"Gustavo Coutinho","userId":"05179387076781997096"}},"outputId":"169c94ee-b9b3-4d3c-b077-0ac3f9a6ff07"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["def standardize(text):\n","    text = unidecode(text.lower())\n","    return \"\".join(char for char in text if char not in string.punctuation)"],"metadata":{"id":"faghjDq9dKok","executionInfo":{"status":"ok","timestamp":1659358319735,"user_tz":180,"elapsed":227,"user":{"displayName":"Gustavo Coutinho","userId":"05179387076781997096"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["standardize(\"Olha só! Como PNL é legal! Estava estudando esses dias.\")"],"metadata":{"id":"63dvbm9MmOx8","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1659358321331,"user_tz":180,"elapsed":232,"user":{"displayName":"Gustavo Coutinho","userId":"05179387076781997096"}},"outputId":"601d4051-7b78-4b0e-aa48-3983dbfce47f"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'olha so como pnl e legal estava estudando esses dias'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["# Tokenização"],"metadata":{"id":"1nKzjz7tdodB"}},{"cell_type":"markdown","source":["Uma tokenização simples consiste na criação de tokens a nível de palavras. Ou seja, com a função ```tokenize()``` vamos criar uma lista de palavras a partir de um texto padronizado com a função ```standardize()```."],"metadata":{"id":"3u97G5-SexS-"}},{"cell_type":"code","source":[""],"metadata":{"id":"PM-bNWNSdsgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"AX102S4fpYfV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Estemização"],"metadata":{"id":"Xz3eS3hyd_mB"}},{"cell_type":"markdown","source":["A estemização vai extrair o radical das palavras. Para isso, utilizaremos o ```PortugueseStemmer()```."],"metadata":{"id":"6zrH5D1WgpCe"}},{"cell_type":"code","source":[""],"metadata":{"id":"nBT79cBIfwrz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"s1p5flyYfxvp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"HRTqkip7f_KP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"oGHfYS10gE_G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"paSk3K3mnt4T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"QD9-XX8QeXph"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"tNpYFj-2eZpI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Lematização"],"metadata":{"id":"q51l9TkTefbB"}},{"cell_type":"markdown","source":["Faremos uma análise mais morfológica das palavras com o processo de lematização. Para isso, utilizaremos a biblioteca Spacy."],"metadata":{"id":"cKJ2OQK3htqr"}},{"cell_type":"code","source":[""],"metadata":{"id":"hyK-kZjVq8da"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"NDsM7kgtfDCJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"vYxsJJ-tfX4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"hI0vVuZofZsB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Indexação do Vocabulário"],"metadata":{"id":"q-7aw6Lwfffm"}},{"cell_type":"markdown","source":["Precisamos transformar as palavras pré-processadas dos nossos textos de entrada em suas representações numéricas.\n","\n","Faremos isso criando um vocabulário das palavras e, em seguida, representando esse vocabulário em formato vetorial com uma codificação *one-hot*."],"metadata":{"id":"NFFcOW0sjc9u"}},{"cell_type":"markdown","source":["## Criação do Vocabulário"],"metadata":{"id":"iXhpldxCgEBl"}},{"cell_type":"code","source":["def make_vocab(texts): \n","    vocab = {\"\": 0, \"[UNK]\": 1} \n","    \n","    for text in texts:\n","      text = standardize(text) \n","      tokens = tokenize(text) \n","      \n","      for token in tokens:\n","        if token not in vocab: \n","          vocab[token] = len(vocab)\n","\n","    inverse_vocab = dict(\n","      (v, k) for k, v in vocab.items())\n","      \n","    return vocab, inverse_vocab"],"metadata":{"id":"90PQ505uvwHm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"nUUMNzvpf1eE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Codificação *One-Hot*"],"metadata":{"id":"Us0mip5egHaN"}},{"cell_type":"code","source":["def one_hot_encode(text, vocab):\n","    tokens = tokenize(text)\n","    vectors = np.zeros((len(tokens), len(vocab)))\n","    \n","    for i, token in enumerate(tokens):\n","      token_idx = vocab.get(token, 1)\n","      vectors[i, token_idx] = 1  \n","    \n","    return vectors"],"metadata":{"id":"coAs2dgBvxD9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2-ZS1C-Jgehj"},"execution_count":null,"outputs":[]}]}